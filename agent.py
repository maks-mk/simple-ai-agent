import json
import asyncio
import logging
import re
from pathlib import Path
from datetime import datetime
from typing import List, Literal, TypedDict, Annotated, Optional, Any

# --- LANGCHAIN & LANGGRAPH ---
from langchain_core.language_models import BaseChatModel
from langchain_core.tools import tool, BaseTool
# –î–û–ë–ê–í–õ–ï–ù ToolMessage –í –ò–ú–ü–û–†–¢–´
from langchain_core.messages import BaseMessage, SystemMessage, RemoveMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.utils.function_calling import convert_to_openai_tool
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

# --- PROVIDERS ---
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

# --- CONFIG & UTILS ---
from pydantic import Field, SecretStr, model_validator
from pydantic_settings import BaseSettings, SettingsConfigDict
from dotenv import load_dotenv

# --- LOCAL MODULES ---
try:
    from logging_config import setup_logging
    logger = setup_logging() 
except ImportError:
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger("agent")

# –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
try:
    from delete_tools import SafeDeleteFileTool, SafeDeleteDirectoryTool
except ImportError:
    SafeDeleteFileTool = SafeDeleteDirectoryTool = None
    
try:
    from search_tools import web_search, fetch_url
except ImportError:
    web_search = fetch_url = None
    logger.warning("Search tools not found or dependencies missing (httpx, bs4).")

try:
    from langchain_mcp_adapters.client import MultiServerMCPClient
except ImportError:
    MultiServerMCPClient = None
    
# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
try:
    import tiktoken
except ImportError:
    tiktoken = None


# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ==========================================

class AgentConfig(BaseSettings):
    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', extra='ignore')

    provider: Literal["gemini", "openai"] = "gemini"
    
    # Keys
    gemini_api_key: Optional[SecretStr] = None
    gemini_model: str = "gemini-1.5-flash"
    openai_api_key: Optional[SecretStr] = None
    openai_model: str = "gpt-4o"
    openai_base_url: Optional[str] = None

    temperature: float = 0.2
    
    # Logic
    use_long_term_memory: bool = Field(default=False, alias="LONG_TERM_MEMORY")
    max_loops: int = Field(default=15, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –∞–≥–µ–Ω—Ç–∞ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å")
    
    # Summarization
    summary_threshold: int = Field(default=20, alias="SESSION_SIZE")
    summary_keep_last: int = Field(default=4, alias="SUMMARY_KEEP_LAST")
    
    # Paths
    prompt_path: Path = Field(default=Path("prompt.txt"), alias="PROMPT_PATH")
    mcp_config_path: Path = Path("mcp.json")
    memory_db_path: str = "./memory_db"

    @model_validator(mode='after')
    def check_api_keys(self) -> 'AgentConfig':
        if self.provider == "gemini" and not self.gemini_api_key:
            raise ValueError("GEMINI_API_KEY required for gemini provider.")
        if self.provider == "openai" and not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY required for openai provider.")
        return self

    def get_llm(self) -> BaseChatModel:
        if self.provider == "gemini":
            return ChatGoogleGenerativeAI(
                model=self.gemini_model,
                temperature=self.temperature,
                google_api_key=self.gemini_api_key.get_secret_value(),
                convert_system_message_to_human=True
            )
        elif self.provider == "openai":
            return ChatOpenAI(
                model=self.openai_model,
                temperature=self.temperature,
                api_key=self.openai_api_key.get_secret_value(),
                base_url=self.openai_base_url,
                model_kwargs={"stream_options": {"include_usage": True}}
            )
        raise ValueError(f"Unknown provider: {self.provider}")


# ==========================================
# 2. –°–û–°–¢–û–Ø–ù–ò–ï –ì–†–ê–§–ê
# ==========================================

class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    summary: str
    steps: int


# ==========================================
# 3. WORKFLOW
# ==========================================

class AgentWorkflow:
    def __init__(self):
        load_dotenv()
        self.config = AgentConfig()
        self.tools: List[BaseTool] = []
        self.llm: Optional[BaseChatModel] = None
        self.llm_with_tools: Optional[BaseChatModel] = None
        self._encoder = None

    async def initialize_resources(self):
        logger.info(f"Initializing agent: [bold cyan]{self.config.provider}[/]", extra={"markup": True})
        self.llm = self.config.get_llm()

        if SafeDeleteFileTool and SafeDeleteDirectoryTool:
            cwd = Path.cwd()
            self.tools.extend([
                SafeDeleteFileTool(root_dir=cwd),
                SafeDeleteDirectoryTool(root_dir=cwd)
            ])
            
        # --- –î–û–ë–ê–í–õ–Ø–ï–ú –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ –ü–û–ò–°–ö–ê ---
        if web_search and fetch_url:
            self.tools.extend([web_search, fetch_url])
            logger.info("‚úÖ Search tools loaded.")

        if self.config.use_long_term_memory:
            self._init_memory_tools()

        if MultiServerMCPClient and self.config.mcp_config_path.exists():
            await self._init_mcp_tools()

        # –ü—Ä–∏–≤—è–∑–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∫ LLM
        if self.tools:
            self.llm_with_tools = self.llm.bind_tools(self.tools)
        else:
            self.llm_with_tools = self.llm
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–Ω–∫–æ–¥–µ—Ä–∞ –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
        if tiktoken:
            try:
                self._encoder = tiktoken.get_encoding("cl100k_base")
            except Exception:
                pass

    def _init_memory_tools(self):
        try:
            from memory_manager import MemoryManager
            memory = MemoryManager(db_path=self.config.memory_db_path)
            
            @tool
            async def remember_fact(text: str, category: str = "general") -> str:
                """Saves an important fact about the user, project, or preferences."""
                return await memory.aremember(text, {"type": category})
            
            @tool
            async def recall_facts(query: str) -> str:
                """Searches for information in long-term memory."""
                facts = await memory.arecall(query)
                return "\n".join(f"- {f}" for f in facts) if facts else "No facts found."
            
            @tool
            async def forget_fact(query: str) -> str:
                """Removes facts from memory."""
                return f"Forgotten: {await memory.adelete_fact_by_query(query)}"

            self.tools.extend([remember_fact, recall_facts, forget_fact])
        except ImportError:
            logger.warning("MemoryManager not found.")

    async def _init_mcp_tools(self):
        if not self.config.mcp_config_path.exists(): return
        try:
            raw_cfg = json.loads(self.config.mcp_config_path.read_text("utf-8"))
            mcp_cfg = {
                name: {
                    **{k: v for k, v in cfg.items() if k != 'enabled'},
                    "args": [a.replace("{filesystem_path}", str(Path.cwd())) for a in cfg.get("args", [])]
                }
                for name, cfg in raw_cfg.items() if cfg.get("enabled", True)
            }
            if mcp_cfg:
                client = MultiServerMCPClient(mcp_cfg)
                new_tools = await asyncio.wait_for(client.get_tools(), timeout=120)
                self.tools.extend(new_tools)
                logger.info(f"Loaded MCP tools: {list(mcp_cfg.keys())}")
        except Exception as e:
            logger.error(f"MCP Error: {e}")

    def _get_base_prompt(self) -> str:
        if self.config.prompt_path.exists():
            raw_prompt = self.config.prompt_path.read_text("utf-8")
        else:
            raw_prompt = (
                "You are an autonomous AI agent with access to tools.\n"
                "Always fulfill the user's request.\n"
                "Your internal reasoning and tool usage must be in English.\n"
                "HOWEVER, your final response to the user must be in Russian.\n"
                "Current date: {{current_date}}\n"
                "CWD: {{cwd}}"
            )

        prompt = raw_prompt.replace("{{current_date}}", datetime.now().strftime("%Y-%m-%d"))
        prompt = prompt.replace("{{cwd}}", str(Path.cwd()))
        
        if self.config.use_long_term_memory:
             prompt += "\nUse memory tools (recall_facts/remember_fact) when necessary."
        
        return prompt

    def _count_tokens(self, text: str) -> int:
        if not text: return 0
        if self._encoder:
            return len(self._encoder.encode(text))
        return len(text) // 3  # Fallback heuristic

    def _estimate_payload_tokens(self, messages: List[BaseMessage], tools: List[BaseTool]) -> int:
        """
        –°—á–∏—Ç–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: —Å–æ–æ–±—â–µ–Ω–∏—è + —Å—Ö–µ–º—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.
        """
        total = 0
        # 1. –°—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        for m in messages:
            content = m.content if isinstance(m.content, str) else ""
            if isinstance(m.content, list):
                content = " ".join([str(x) for x in m.content])
            total += self._count_tokens(content)
        
        # 2. –°—á–∏—Ç–∞–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã (JSON schema)
        if tools:
            try:
                tool_schemas = [convert_to_openai_tool(t) for t in tools]
                tools_json = json.dumps(tool_schemas, ensure_ascii=False)
                total += self._count_tokens(tools_json)
            except Exception:
                simple_desc = "\n".join([f"{t.name}: {t.description}" for t in tools])
                total += self._count_tokens(simple_desc)
        
        return total

    # ==========================================
    # 4. –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò (SANITIZER)
    # ==========================================
    
    def _sanitize_path(self, path: str) -> str:
        """
        –ñ–µ—Å—Ç–∫–æ —á–∏—Å—Ç–∏—Ç –ø—É—Ç—å –æ—Ç –¥–≤–æ–µ—Ç–æ—á–∏–π, –∫–∞–≤—ã—á–µ–∫ –∏ —è–∑—ã–∫–æ–≤—ã—Ö –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤.
        –ü—Ä–∏–º–µ—Ä: ":ru:file.txt" -> "file.txt"
        """
        original = path
        path = re.sub(r'^:[a-z]{2,3}:', '', path) # :ru:
        path = re.sub(r'[:"<>|?*]+', '', path)    # Win chars
        path = path.strip()
        
        if path != original:
            # logger.warning(f"üõ°Ô∏è Path sanitized: '{original}' -> '{path}'")
            # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å debug, —á—Ç–æ–±—ã –Ω–µ —à—É–º–µ—Ç—å –≤ –∫–æ–Ω—Å–æ–ª—å
            logger.debug(f"üõ°Ô∏è Path sanitized: '{original}' -> '{path}'")
        
        return path

    def _fix_tool_calls(self, tool_calls: List[dict]):
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–∞–Ω–∏—Ç–∞–π–∑–µ—Ä –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.
        1. –ß–∏—Å—Ç–∏—Ç –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º (File Tools).
        2. –ß–∏—Å—Ç–∏—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç URL (Web Tools), –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—è loop —Å '}'.
        """
        path_keys = {"path", "file_path", "dir_path", "destination", "source", "filename"}
        url_keys = {"url", "link", "target_url", "query"} 

        for tc in tool_calls:
            args = tc.get("args")
            name = tc.get("name")

            # –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ –∑–Ω–∞—á–µ–Ω–∏—è
            def clean_val(k, v):
                if not isinstance(v, str): return v
                
                # A. –õ–æ–≥–∏–∫–∞ –¥–ª—è –ø—É—Ç–µ–π
                if k in path_keys:
                    return self._sanitize_path(v)
                
                # B. –õ–æ–≥–∏–∫–∞ –¥–ª—è URL (–∑–∞—â–∏—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞)
                if name == "fetch_url" and (k in url_keys or k == "url"):
                    # –£–¥–∞–ª—è–µ–º –∫–∞–≤—ã—á–∫–∏, —Å–∫–æ–±–∫–∏, —Ñ–∏–≥—É—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏ –ò –î–í–û–ï–¢–û–ß–ò–Ø
                    clean = v.strip().strip("'").strip('"').strip("{}").strip(":")
                    
                    # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª–∞—Å—å –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ –º—É—Å–æ—Ä, –Ω–æ –≤–Ω—É—Ç—Ä–∏ –µ—Å—Ç—å http - —Å–ø–∞—Å–∞–µ–º
                    if "http" in v and not clean.startswith("http"):
                        match = re.search(r'(https?://[^\s\'"<>{}]+)', v)
                        if match:
                            clean = match.group(1)
                            logger.debug(f"üõ°Ô∏è URL extracted: '{v}' -> '{clean}'")
                    
                    return clean
                
                return v

            # 1. –ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã (Dict)
            if isinstance(args, dict):
                for key, value in args.items():
                    args[key] = clean_val(key, value)

            # 2. –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã (List)
            elif isinstance(args, list) and len(args) > 0:
                if isinstance(args[0], str):
                    # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ fetch_url, —Ç–æ –ø–µ—Ä–≤—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç - url, –∏–Ω–∞—á–µ - –ø—É—Ç—å
                    fake_key = "url" if name == "fetch_url" else "path"
                    args[0] = clean_val(fake_key, args[0])

    # ==========================================
    # 5. –£–ó–õ–´ –ì–†–ê–§–ê
    # ==========================================

    async def _summarize_node(self, state: AgentState):
        messages = state["messages"]
        summary = state.get("summary", "")

        if len(messages) <= self.config.summary_threshold:
            return {}

        keep_last = self.config.summary_keep_last
        idx = len(messages) - keep_last
        while idx < len(messages):
            if isinstance(messages[idx], HumanMessage):
                break
            idx += 1
        
        to_summarize = messages[:idx]
        if not to_summarize:
            return {}

        history_text = "\n".join([f"{m.type}: {m.content}" for m in to_summarize])

        prompt = (
            f"Current memory context:\n<previous_context>\n{summary}\n</previous_context>\n\n"
            f"New events:\n{history_text}\n\n"
            "Update <previous_context>. Keep only key facts, decisions, and results. "
            "Remove chit-chat. Return only the updated context text."
        )
        
        try:
            res = await self.llm.ainvoke(prompt)
            delete_msgs = [RemoveMessage(id=m.id) for m in to_summarize if m.id]
            logger.info(f"üßπ –°–∂–∞—Ç–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: —É–¥–∞–ª–µ–Ω–æ {len(delete_msgs)} —Å–æ–æ–±—â–µ–Ω–∏–π.")
            return {"summary": res.content, "messages": delete_msgs}
        except Exception as e:
            logger.error(f"Summarization Error: {e}")
            return {}

    async def _agent_node(self, state: AgentState):
        messages = state["messages"]
        summary = state.get("summary", "")
        
        # 1. –§–æ—Ä–º–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        sys_text = self._get_base_prompt()
        if summary:
            sys_text += f"\n\n<previous_context role='memory' priority='low'>\n{summary}\n</previous_context>"
        
        sys_msg = SystemMessage(content=sys_text)
        history = [m for m in messages if not isinstance(m, SystemMessage)]
        full_context = [sys_msg] + history
        
        # 2. [FALLBACK] –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å—á–∏—Ç–∞–µ–º Input —Ç–æ–∫–µ–Ω—ã
        estimated_input = self._estimate_payload_tokens(full_context, self.tools)
        response = None
        
        # 3. RETRY LOGIC: –¶–∏–∫–ª –ø–æ–ø—ã—Ç–æ–∫ –≤—ã–∑–æ–≤–∞ LLM
        for attempt in range(3):
            try:
                response = await self.llm_with_tools.ainvoke(full_context)
                
                # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø—É—Å—Ç–æ–π
                if not response.content and not response.tool_calls:
                    raise ValueError("Empty response received from LLM")
                
                break # –£—Å–ø–µ—Ö
                
            except Exception as e:
                # --- SMART FIX –î–õ–Ø write_file ---
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –≤–æ–∑–Ω–∏–∫–ª–∞, –Ω–æ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏–∏ - —ç—Ç–æ —É—Å–ø–µ—à–Ω–∞—è –∑–∞–ø–∏—Å—å —Ñ–∞–π–ª–∞,
                # –∑–Ω–∞—á–∏—Ç –º–æ–¥–µ–ª—å –ø—Ä–æ—Å—Ç–æ "–ø–æ–ª–µ–Ω–∏–ª–∞—Å—å" –æ—Ç–≤–µ—Ç–∏—Ç—å. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –∑–∞ –Ω–µ—ë.
                last_msg = full_context[-1] if full_context else None
                is_write_success = (
                    isinstance(last_msg, ToolMessage) 
                    and "Successfully wrote" in str(last_msg.content)
                )
                
                if is_write_success:
                    logger.info("üõ°Ô∏è Auto-completing after write_file crash.")
                    response = AIMessage(
                        content="–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω. –ó–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞. (–ê–≤—Ç–æ-–∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ)"
                    )
                    break
                # --------------------------------
                
                logger.warning(f"‚ö†Ô∏è LLM Error (Attempt {attempt+1}/3): {e}")
                if attempt == 2:
                    response = AIMessage(
                        content=f"System Error: The model produced invalid output after 3 attempts. Error: {e}"
                    )
                else:
                    await asyncio.sleep(1)
                    
        # 4. --- –ò–ù–¢–ï–†–°–ï–ü–¢–û–†: –ß–ò–°–¢–ö–ê –ê–†–ì–£–ú–ï–ù–¢–û–í ---
        if response.tool_calls:
            self._fix_tool_calls(response.tool_calls)

        # 5. --- QUALITY GATE: –ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞–ø–∏—Å–∏ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤ ---
        if response.tool_calls and any(tc['name'] == 'write_file' for tc in response.tool_calls):
            has_valid_data = False
            for m in history:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º ToolMessage –∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤!
                if isinstance(m, ToolMessage) and m.name in ["fetch_url", "web_search"]:
                    content_str = str(m.content)
                    if "Error" not in content_str and "–û—à–∏–±–∫–∞" not in content_str and len(content_str) > 100:
                        has_valid_data = True
                        break
            
            if not has_valid_data:
                logger.warning("üõ°Ô∏è Quality Gate: Blocked write_file due to lack of valid sources.")
                response = AIMessage(
                    content="STOP. You are trying to write a file, but ALL your previous search/fetch attempts failed or returned errors. "
                            "You have NO valid data to write. You MUST try searching again with different keywords or fetch different URLs first."
                )

        # 6. [FALLBACK] –ü–∞—Ç—á –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        usage = response.usage_metadata or {}
        input_tokens = usage.get("input_tokens", 0)
        
        if input_tokens == 0:
            output_content = response.content
            if isinstance(output_content, list):
                output_content = " ".join([str(x) for x in output_content])
            
            estimated_output = self._count_tokens(str(output_content))
            
            if response.tool_calls:
                tools_str = json.dumps([tc for tc in response.tool_calls], default=str)
                estimated_output += self._count_tokens(tools_str)
            
            new_meta = {
                "input_tokens": estimated_input,
                "output_tokens": estimated_output,
                "total_tokens": estimated_input + estimated_output
            }
            
            try:
                response.usage_metadata = new_meta
            except Exception:
                response = AIMessage(
                    content=response.content,
                    tool_calls=response.tool_calls,
                    id=response.id,
                    response_metadata=response.response_metadata,
                    usage_metadata=new_meta
                )

        return {"messages": [response]}
        
    async def _loop_guard_node(self, state: AgentState):
        return {
            "messages": [
                AIMessage(
                    content=(
                        "üõë **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞**\n\n"
                        "–ê–≥–µ–Ω—Ç –ø—Ä–µ–≤—ã—Å–∏–ª –ª–∏–º–∏—Ç —à–∞–≥–æ–≤."
                    )
                )
            ]
        }

    def build_graph(self):
        workflow = StateGraph(AgentState)

        workflow.add_node("summarize", self._summarize_node)
        workflow.add_node("agent", self._agent_node)
        workflow.add_node("loop_guard", self._loop_guard_node)
        workflow.add_node("update_step", lambda state: {"steps": state.get("steps", 0) + 1})
        
        if self.tools:
            workflow.add_node("tools", ToolNode(self.tools))

        workflow.add_edge(START, "summarize")
        workflow.add_edge("summarize", "update_step") 
        workflow.add_edge("update_step", "agent")

        def should_continue(state):
            steps = state.get("steps", 0)
            if steps >= self.config.max_loops:
                logger.warning(f"üõë Loop Guard triggered: {steps} steps.")
                return "loop_guard" 

            last_msg = state["messages"][-1]
            return "tools" if getattr(last_msg, "tool_calls", None) else END

        destinations = ["tools", "loop_guard", END] if self.tools else ["loop_guard", END]
        workflow.add_conditional_edges("agent", should_continue, destinations)

        if self.tools:
            workflow.add_edge("tools", "update_step")

        workflow.add_edge("loop_guard", END)

        return workflow.compile(checkpointer=MemorySaver())
        
if __name__ == "__main__":
    async def main():
        wf = AgentWorkflow()
        await wf.initialize_resources()
        wf.build_graph()
        print(f"‚úÖ Agent Ready. Tools: {len(wf.tools)}")

    asyncio.run(main())